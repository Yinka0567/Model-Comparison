# Model-Comparison
**Final Year Project as an undegraduate of a Student of University of Ilorin Studying Statistics**

**A Comparative Performance Analysis of Some Selected Classification Models**
## Introduction
In the dynamic landscape of data classification, the choice of an appropraite ML model plays a crucial in determining prediction accuracy. This study presents a comprehensive comparison of several widely used models: Logistic Regression, K-Nearest Neighbors(KNN), Naive Bayes, Random Forest, and Decision Tree. These models have attracted considerable attention due to their diverse applications and unique characteristics.
 **Logistic Regression**,a fundamental algorithm in supervised learning, is valued for its simplicity and interpretability [Bui et al., 2020]. It is particularly effective for binary classification tasks, utilizing a linear decision boundary to separate classes within a dataset.
 **K-Nearest Neighbors (KNN)** is a non-parametric algorithm known for its intuitive approach to classification. It predicts the class of a data point based on the majority class among its closest neighbors. KNN is highly effective when sufficient data is available, and it performs particularly well with larger sample sizes due to its sensitivity to local data structure [Alsabhan et al., 2022].
**Naïve Bayes**, a probabilistic classifier based on Bayes’ Theorem, assumes feature independence. While this assumption may not always hold in real-world scenarios, the model is computationally efficient and performs surprisingly well with small datasets [Bui et al., 2020]. However, its performance tends to decline as dataset complexity and volume increase.
**Random Forest**, an ensemble learning method introduced by Breiman [Breiman, 2001], aggregates the results of multiple decision trees to enhance accuracy and reduce overfitting. It is known for its robustness, especially when dealing with high-dimensional data or complex interactions among features. Nevertheless, its performance may vary depending on the size and nature of the dataset.
**Decision Tree**, although simple and interpretable, often suffers from high variance and overfitting, particularly with small datasets. However, as sample size increases, its predictive performance tends to improve significantly, making it a viable option when adequate data is available [Li et al., 2020].
This study evaluates these five models using performance metrics such as Precision, Recall, F1 Score, and AUC across varying sample sizes. By examining their behavior under different data conditions, this research aims to guide practitioners in selecting suitable models for effective and scalable classification tasks.
## Statement Problem
Choosing an appropriate classification model is a critical challenge in statistical modeeling and machine learning. With the vast array of algorithms available- each designed with different assumptions, computational complexities, and data requirements, selecting the most suitable model for a given dataset is far from striaghtforward. Practitioners frequently default to well-known models or rely on past experience and software tools, often without a thorough understanding of how model performance can vary under different data conditions.  Moreover, the performance of ML models is often evaluated using accuracy, which may provide a misleading picture, especially in imbalanced datasets where one class dominates. In such cases, metrics like precision, Recall, F1 Score and AUC (Area Under the Curve) provide a more holistic view of performance (Saito & Rehmsmeier, 2015; Powers, 2011). However, Many comparative studies continue to overlook these comprehensive metrics. Despite the growing application of ML in sensitive and high-stakes domains such as healthcare, finance, law enforcement, and socail science, there is alack of systematic studies comparing standard classification algorithms under controlled and replicable conditions. Most existing comparisons are conducted on real-world datasets that come with inherent noise, missing data, and unbalanced distiributions--factors that can obscure meaningful insights about model behavior.

 To address these gaps, this study  adopts a simulation-based approach that allows full control over key data generation parameters, such as a feature independence, outcome distributions, and sample size. This design enables a focused comparison of five commonly used classification models; Logistic Regression, Naive Bayes, Random Forest, K-Nearest Neighbours, and Decision Tree, across sample sizes ofv 100, 500, 1000, and 2000 observations. The use of multiple performance metrics ensure a comprehensive evaluation of each models's strengths and limitation.

 This study focuses on a simulation-based comparative analysis of five aforementioned classification models. Through the controlled generation of synthetic datasets with varying sample sizes and known properties, the study aims to evaluate and compare model performance using standard evaluation metrics. This approach allows fot the isolation of key vaariables and provide insights into how different models scale aand generalize across differnt data conditions

 ## Aim and Objectives of the Study
 ### Aim
 To conduct a simulation-based evaluation of five selected machine learning classification models and compare their performance across different sample sizes using standard performance metrics.
 ### Objectives
 To achieve the stated aim, the study will:
1.	To develop and apply five supervised classification models: logistics regression, naive bayes, k-nearest neighbors, decision tree, and random forest, using synthetically generated datasets that serve controlled statistical conditions in ensuring consistency and comparability in model evaluation.
2.	Evaluate the classification performance of each model using Precision, Recall, F1 Score, and AUC (Area Under the Curve) and investigate how each model’s performance changes as the sample size increase) n= 100, 500, 1000,2000)
## Scope of the Study
This study is strictly limited to binary classification problems under simulated data conditions. The scope includes:
1.	Data Generation: Ten predictor variables (X_1  to X_10) will be generated from a multivariate normal distribution with mean of 0 and variance of 1, assuming independence among features
2. Target Variable: The binary outcome (Y) is generated using a logistic function applied to a weighted sum of the predictors, with β coefficients randomly selected between -0.5 and 0.5.
3. Sample Sizes: Datasets will be generated for four different sample sizes; 100, 500, 1000, and 2000 observations
4. Classification Models: Five Models will be studied; Logistics Regression, Naïve Bayes, K-Nearest Neighbors, Decision Tree, and Random Forest.
5. Performance Metrics: Each model will be evaluated using Precision, Recall, F1 Score, and Area Under the Curve (AUC)

- Precision: This is the proportion of true positive predictions out of all positive predictions made by the model. It simply measures the accuracy of positive predictions. To correctly identify any false positive in the prediction.
- Recall: This is also known as sensitivity/true positive rate is the proportion of true positive predictions from all actual positive samples in the dataset. It measures the model’s ability to identify all positive  instances and is critical when the cost of false negative is high. A higher recall means the model is capturing more actual positive, which means fewer false negative.
- F1 Score: This is a measure of model’s accuracy that takes into account both precision and recall, where the goal is to classify instances correctly as positive or negative. A high --F1 score indicates a balanced performance across precision and recall for a given binary classification problem
- AUC: Area under the ROC curve, measuring overall performance of a classification model by calculating the area under the ROC curve (Receiver Operating Characteristic curve). AUC summarizes this True Positive Rate(TPR) and False Negative Rate (FPR) into a single number that tells us how well the model separates the positive class from the negative class. An AUC close to 1 indicates excellent separation, while 0.5 indicates random guessing, less than 0.5 indicates model perform worse than random.
## Data Analysis and Interpretation
